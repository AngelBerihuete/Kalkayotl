{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a basic tutorial to run Kalkayotl\n",
    "To be able to use Kalkayotl you will need to follow the installation steps provided in the GitHub [page](https://github.com/olivares-j/Kalkayotl), and activate its environement.\n",
    "\n",
    "You can also directly lunch the `example.py` code once you have adaptted it according to your cluster data and characteristics.\n",
    "\n",
    "First, we load the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl'\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, unicode_literals, print_function\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from kalkayotl import Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are any error review the installation steps of Kalkayotl and/or Jupyterlab.\n",
    "\n",
    "Next we define the directory and data file. We create it if it does not exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_out    = os.getcwd() + \"/Example/\"\n",
    "os.makedirs(dir_out,exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide the data file. Currently it is only supported the CSV file. The names of the columns must be those of the standard Gaia DR2. The only exception is the source ID, by default it is set to \"source_id\" as in Gaia data, but you will be able to provide another name below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_data = dir_out + \"Ruprecht_147.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knobs\n",
    "Now we define some of the code parameters.\n",
    "\n",
    "### Sampler parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Chains_: it refers to the number of HMC chains that the sampler will create. To analyse convergence we need at least two. More chains will provide more samples but will also consume more resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chains = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Cores_: it referes to the number of computers cores or processors that will be used to run the HMC sampler. The best performance will vary in different machines. The best option is to use one core per chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores  = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Burning_iterations_: it refers to the number of iterations that the HMC sampler will use to adapt its parameters. The rule here is that more iterations will help to improve the performance of the sampler. This is a parameter that you may need to increse if convergence problems arise. These samples will be discarded to avoid biasing the parameters estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "burning_iters   = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Sample_iterations_: it refers to the number of actual samples that will be deliverd by the HMC sampler. As explained in the paper, the number of samples depend on the precision that you need for the model parameters. More samples improve the precision but also take more time. Adapt this value according to your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_iters    = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two parameters refer to the initialization of the sampler positions. These are the initial positions of the HMC chains. Although theoretically the sampler must converge in spite of the values of the intial positions, in practice if these are far away from the true parameter values, then the likelihood is not able improve by small movements of the parameters positions, and the sampler is basically lost and takes a lot of time to converge. Since we do not want to waste our time, we provide the sampler with a roughly good starting point.\n",
    "\n",
    "After testing several of the initialization modes provided by PyMC3, I found that the best one is the 'advi+adapt_diag' with 500000 iterations. This scheme performs variational inference to find the best parameters positions. Although it is the best of the initialization schemes, for the particular case of Kalkyotl, it is still prone to failures. In some cases if one or several of the distances to the stars fail to fall within the \"field-of-view\" of its parallax uncertainties (i.e. fall beyond 5-sigma) then the initialization will fail with errors like: \"Bad initial energy\" or \".rvel is zero\". In this cases the best is to launch the code again. Hopefully the initial position will fall closer to the \"field-of-view\". This type of problem is more recurrent in models with high number of parameters (i.e. stars)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_mode = 'advi+adapt_diag'\n",
    "init_iter = 500000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Target_acceptance_: it refers to an internal parameter of the HMC. It is recomended to be larger than 0.7. Smaller values are recomended for simpler problems while increasing its values helps to improve the convergence of the sampler but also increases the computing time. Usually the more complex prior families require a larger value. For this reason this parameter is defined for each prior family. But you are free to increse its value in case of convergence problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_accept = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output statistics\n",
    "\n",
    "_Statistics_: It refers to the type of statistics that will be computed from the posterior samples. Options are \"mean\",\"median\" and \"mode\". The quantiles refer to the lower and uper values of the uncertainties. One sigma uncertainties will correspond to [0.16,0.84], while two sigma uncertainties will be [0.025,0.975]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistic = \"mean\"\n",
    "quantiles = [0.025,0.975]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model configuration\n",
    "\n",
    "_Transformation_: it refers to the space in which Kalkayotl will work, either in the distance space (choose \"pc\") or the parallax space (choose \"mas\"). These units will be the same in which the hyper-parameters must be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation = \"pc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Zero_point_: it refers to the parallax zero point of the Gaia data. You can provide either a scalar or a vector of the same dimension as the valid sources in your data set. We use the Lindegren+2018 value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_point = -0.029 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Parametrization_: it refers to the type of parametrization of the Hierarchical model. It is known that this type of models face problems when its parameters are inferred using HMC samplers. To improve parformance two types of parameterizations are provided \"central\" and \"non-central\". While the first one works better when the data set is highly informative (nearby clusters stars with narrow parallax uncertainties), the last one works better for low informative data sets, like those of the farthest stars and clusters. In the case of Ruprecht 147 we use the central parameterizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametrization=\"central\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Independent_measurements_: In the Gaia astrometric data the measurements of stars are spatially correlated.\n",
    "This parameter controls if the data is assumed to be independent (i.e. the spatial correlations are neglected) or not (i.e. the spatiall correlations are taken into account. Setting indep_measures=False implies that the spatial correlations will be taken into account. The default model for this correlations is the one provided by Vasiliev et al. 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "indep_measures = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior configuration\n",
    "\n",
    "Here we will only show ohow to configure the King prior family. The rest of the families are configured in a similar way since most of the parameters are shared. \n",
    "\n",
    "_Type_: We use a valid prior family name: \"Uniform\", \"Gaussian\", \"King\", \"EFF\", or \"GMM\".\n",
    "\n",
    "_Parameters_: It must be a dictionary with the names of the parameters (see the `example.py` file for the names of other prior parameters). You can either use a number, in which case the parameter will be fixed to that value throgughout the inference, or set it to None, in this case the value will be inferred as well.\n",
    "\n",
    "_Hyper_alpha_: It refers to the hyper-parameter of the location prior. You must provide a list with the location and scale of the Gaussian distribution that will be used as prior for the location parameter (i.e. the distance).\n",
    "\n",
    "_Hyper_beta_: It corresponds to the hyper-parameter of the scale prior. It correspond to the typicall size of the cluster. Here we use a rather large value. But if you have more constrianing inofrmation use it if you face convergence problems.\n",
    "\n",
    "_Hyper_gamma_: It corresponds to the hyper-parameter of the prior for the tidal radius. Is similar to the hyper_beta but it is now expressed in units of core radius and restricted to be larger than one. \n",
    "\n",
    "_Hyper_delta_: It is only used in the GMM prior family so we set it to None.\n",
    "\n",
    "_Burning_iters_: This is the number of iterations. We keep it within the prior for compatibility to the `example.py` file, where all prior families can be run at once.\n",
    "\n",
    "_Target_accept_: Similarly to the previous one this parameter is kept within the prior dictionary for compatibility. It is explained above in the sampler parameters.\n",
    "\n",
    "We set the prior as a dictionary for simplicity, but this is not required by the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = {\n",
    " \"type\":\"King\",         \n",
    " \"parameters\":{\"location\":None,\"scale\":None,\"rt\":None},\n",
    " \"hyper_alpha\":[305.,30.], \n",
    " \"hyper_beta\":[50.], \n",
    " \"hyper_gamma\":[50.],\n",
    " \"hyper_delta\":None,\n",
    " \"burning_iters\":burning_iters,\n",
    " \"target_accept\":target_accept}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Kalkayotl\n",
    "\n",
    "First we create the output directory specific for this prior. Again this is for compatibility with the `example.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_prior = dir_out + prior[\"type\"] + \"/\"\n",
    "os.makedirs(dir_prior,exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the inference module with the model, sampler and prior parameters.\n",
    "\n",
    "For now, Kalkayotl only works for computing distances, so we set dimension to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1d = Inference(dimension=1,\n",
    "                prior=prior[\"type\"],\n",
    "                parameters=prior[\"parameters\"],\n",
    "                hyper_alpha=prior[\"hyper_alpha\"],\n",
    "                hyper_beta=prior[\"hyper_beta\"],\n",
    "                hyper_gamma=prior[\"hyper_gamma\"],\n",
    "                hyper_delta=prior[\"hyper_delta\"],\n",
    "                dir_out=dir_prior,\n",
    "                transformation=transformation,\n",
    "                zero_point=zero_point,\n",
    "                indep_measures=indep_measures,\n",
    "                parametrization=parametrization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the data. Other options for the load_data function beside that of the data file include the `id_name` keyword to use in case you have different ID names as from those in Gaia data. If so you must provide a string containing the column name of the IDs. By default it uses 'source_id'. Finally, the `corr_func` keyword refers to the type of correlation function to use for the spatial correlations. The options are 'Vasiliev+2019', the default one, and 'Lindegren+2018'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Vasiliev+2019 spatial correlation function\n",
      "Data correctly loaded\n"
     ]
    }
   ],
   "source": [
    "p1d.load_data(file_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we configure the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring King prior\n",
      "Using central parametrization.\n"
     ]
    }
   ],
   "source": [
    "p1d.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to run the sampler with all the previous parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing posterior\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using advi+adapt_diag...\n",
      "Average Loss = 4.8951e+18:   4%|▍         | 21299/500000 [00:18<06:44, 1182.77it/s]\n",
      "Convergence achieved at 21300\n",
      "Interrupted at 21,299 [4%]: Average Loss = 3.303e+19\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [1D_source, 1D_x, 1D_scl, 1D_loc]\n",
      "Sampling 2 chains:  43%|████▎     | 10353/24000 [01:20<01:05, 207.81draws/s]"
     ]
    }
   ],
   "source": [
    "p1d.run(sample_iters=sample_iters,\n",
    "\t\tburning_iters=prior[\"burning_iters\"],\n",
    "\t\tinit=init_mode,\n",
    "\t\tn_init=init_iter,\n",
    "\t\ttarget_accept=prior[\"target_accept\"],\n",
    "\t\tchains=chains,\n",
    "\t\tcores=cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases the sampler does not perform well and there are few effective samples and or some divergences. To improve the performance we can increase the number of burning iterations, and/or reparameterize (i.e. provide more constraining priors). A few divergences (<100) in the King prior are not generally a problem. If you want to fully remove them you will have to increase the burning iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the sampler we load the chains. This function comes at hand when you have already run the model\n",
    "but you want to reanalyze or makes some plots without runing again the model. In this latter case simply comment the previous p1d.run() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1d.load_trace(sample_iters=sample_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the chains/traces are loaded we analyse their convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p1d.convergence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gelman-Rubin statistic must be near one, which is the case. Let's taka a look at the plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots\n",
    "\n",
    "Once we are satisfied with the convergence of the sampler we can make further checks by analyzing the trace plots.\n",
    "\n",
    "We start by plotting the chains. The `plot_chains` function takes as optional argument the `IDs` keyword. If not supplied the function will create only plots with the traces of the cluster parameters. If you pass valid IDs as a list of strings it will also create the trace plots for individual sources for which the IDs were provided. The output plots will be in the `dir_out` directory under the name \"Traces.pdf\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1d.plot_chains(IDs=['4087735025198194176'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "file_traces = \"Example/King/Traces.pdf\"\n",
    "IFrame(file_traces,width=700, height=350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows that source '4087735025198194176' has a nice mixing of the two chains. Concerning the cluster parameters we see that the location has still some correlations, which is causing the low effective sample size. Both the scale and rt parameters are fine, with some correlation still. The chains look fine and the Gelman-Rubin is also correct, if you still want to further improve convergence you can try with larger chains and more constraining priors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples and statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kalkayotl can also provide the statistics and quantiles of all parameters in the model. The function `save_statistics` will create two output CSV files, within the `dir_out` directory. These files will be named Sources_{statistic}.csv and Cluster_{statistic}. csv and will contain the statistics of the individual sources and the cluster parameters, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1d.save_statistics(statistic=statistic,quantiles=quantiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also save the samples from the positerior into an HDF5 file. This latter is more practic and compressed than the CSV files. The file will be created in the `dir_out` directory specified in the initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1d.save_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evidence\n",
    "\n",
    "As explained in the paper, the evidence module of Kalkayotl is there only to give you some help when deciding which prior family is the best one for your particular data set. It is computationally expensive, so I recommend to run only in a subsample of your data set (the `M_samples` parameter). The rest of the parameters of the `evidence` function include the number of live points (`nlive`) and the convergence tolerance (`dlogz`). At the end an output file called 'Evidence.csv' will be created in the `dir_out` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1d.evidence(M_samples=1000,dlogz=1.0,nlive=100,file=file_Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract samples\n",
    "\n",
    "Finally, here is a piece of code showing how to extract the samples from the Samples.h5 file. In addition it will print the mean and standard deviation of the samples obtained for each surce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "file_distances = dir_out + \"/King/Samples.h5\"\n",
    "hf = h5py.File(file_distances,'r')\n",
    "srcs = hf.get(\"Sources\")\n",
    "\n",
    "n_samples = 100\n",
    "samples = np.empty((len(srcs.keys()),n_samples))\n",
    "#-------- loop over array and fill it with samples -------\n",
    "for i,ID in enumerate(srcs.keys()):\n",
    "\t#--- Extracts a random choice of the samples --------------\n",
    "\tsamples[i] = np.random.choice(np.array(srcs.get(str(ID))),\n",
    "\t\t\t\t\t\t\tsize=n_samples,replace=False)\n",
    "\t#----------------------------------------------------------\n",
    "\n",
    "\tprint(\"Source {0} at {1:3.1f} +/- {2:3.1f} pc.\".format(ID,\n",
    "\t\t\t\t\t\t\t\t\t\tsamples[i].mean(),\n",
    "\t\t\t\t\t\t\t\t\t\tsamples[i].std()))\n",
    "\n",
    "#- Close HDF5 file ---\n",
    "hf.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
